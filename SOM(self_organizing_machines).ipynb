{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SOM(self organizing machines).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tpathanjali/ML/blob/master/SOM(self_organizing_machines).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "h1PwYdsLGaSI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q sklearn\n",
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pFRaCh4JGhVf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TNHQiQcxGomw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file_id='1vD-8kH_GDYqfVipSjmyJfx-6SreUKoTa'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('df.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N15ZtCIRHQMJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sklearn as sklearn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oL61jFjvHcaV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset=pd.read_csv('df.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FSTkrOwrHs1s",
        "colab_type": "code",
        "outputId": "ad2e7ece-df21-4635-94cf-ca7357c7cbb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "cell_type": "code",
      "source": [
        "dataset.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CustomerID</th>\n",
              "      <th>A1</th>\n",
              "      <th>A2</th>\n",
              "      <th>A3</th>\n",
              "      <th>A4</th>\n",
              "      <th>A5</th>\n",
              "      <th>A6</th>\n",
              "      <th>A7</th>\n",
              "      <th>A8</th>\n",
              "      <th>A9</th>\n",
              "      <th>A10</th>\n",
              "      <th>A11</th>\n",
              "      <th>A12</th>\n",
              "      <th>A13</th>\n",
              "      <th>A14</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15776156</td>\n",
              "      <td>1</td>\n",
              "      <td>22.08</td>\n",
              "      <td>11.460</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1.585</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>100</td>\n",
              "      <td>1213</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15739548</td>\n",
              "      <td>0</td>\n",
              "      <td>22.67</td>\n",
              "      <td>7.000</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>0.165</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>160</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>15662854</td>\n",
              "      <td>0</td>\n",
              "      <td>29.58</td>\n",
              "      <td>1.750</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1.250</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>280</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>15687688</td>\n",
              "      <td>0</td>\n",
              "      <td>21.67</td>\n",
              "      <td>11.500</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>15715750</td>\n",
              "      <td>1</td>\n",
              "      <td>20.17</td>\n",
              "      <td>8.170</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>1.960</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>60</td>\n",
              "      <td>159</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>15571121</td>\n",
              "      <td>0</td>\n",
              "      <td>15.83</td>\n",
              "      <td>0.585</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>1.500</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>15726466</td>\n",
              "      <td>1</td>\n",
              "      <td>17.42</td>\n",
              "      <td>6.500</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>60</td>\n",
              "      <td>101</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>15660390</td>\n",
              "      <td>0</td>\n",
              "      <td>58.67</td>\n",
              "      <td>4.460</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>8</td>\n",
              "      <td>3.040</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>43</td>\n",
              "      <td>561</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>15663942</td>\n",
              "      <td>1</td>\n",
              "      <td>27.83</td>\n",
              "      <td>1.000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>3.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>176</td>\n",
              "      <td>538</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>15638610</td>\n",
              "      <td>0</td>\n",
              "      <td>55.75</td>\n",
              "      <td>7.080</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>6.750</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>100</td>\n",
              "      <td>51</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   CustomerID  A1     A2      A3  A4  A5  A6     A7  A8  A9  A10  A11  A12  \\\n",
              "0    15776156   1  22.08  11.460   2   4   4  1.585   0   0    0    1    2   \n",
              "1    15739548   0  22.67   7.000   2   8   4  0.165   0   0    0    0    2   \n",
              "2    15662854   0  29.58   1.750   1   4   4  1.250   0   0    0    1    2   \n",
              "3    15687688   0  21.67  11.500   1   5   3  0.000   1   1   11    1    2   \n",
              "4    15715750   1  20.17   8.170   2   6   4  1.960   1   1   14    0    2   \n",
              "5    15571121   0  15.83   0.585   2   8   8  1.500   1   1    2    0    2   \n",
              "6    15726466   1  17.42   6.500   2   3   4  0.125   0   0    0    0    2   \n",
              "7    15660390   0  58.67   4.460   2  11   8  3.040   1   1    6    0    2   \n",
              "8    15663942   1  27.83   1.000   1   2   8  3.000   0   0    0    0    2   \n",
              "9    15638610   0  55.75   7.080   2   4   8  6.750   1   1    3    1    2   \n",
              "\n",
              "   A13   A14  Class  \n",
              "0  100  1213      0  \n",
              "1  160     1      0  \n",
              "2  280     1      0  \n",
              "3    0     1      1  \n",
              "4   60   159      1  \n",
              "5  100     1      1  \n",
              "6   60   101      0  \n",
              "7   43   561      1  \n",
              "8  176   538      0  \n",
              "9  100    51      0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "6A3y0XapHtmU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X=dataset.iloc[:,:-1].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yeEMhwlFLkYK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y=dataset.iloc[:,-1].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1O-zY6lSLvTh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#feature scaling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sc = MinMaxScaler(feature_range=(0,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mHVm1xR1L34C",
        "colab_type": "code",
        "outputId": "853cb3b3-2790-4025-e88d-bca023b8d435",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "cell_type": "code",
      "source": [
        "sc.fit_transform(X)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[8.42681467e-01, 1.00000000e+00, 1.25263158e-01, ...,\n",
              "        5.00000000e-01, 5.00000000e-02, 1.21200000e-02],\n",
              "       [6.96090562e-01, 0.00000000e+00, 1.34135338e-01, ...,\n",
              "        5.00000000e-01, 8.00000000e-02, 0.00000000e+00],\n",
              "       [3.88981656e-01, 0.00000000e+00, 2.38045113e-01, ...,\n",
              "        5.00000000e-01, 1.40000000e-01, 0.00000000e+00],\n",
              "       ...,\n",
              "       [4.39420332e-01, 0.00000000e+00, 7.63909774e-02, ...,\n",
              "        5.00000000e-01, 5.00000000e-02, 0.00000000e+00],\n",
              "       [8.44034934e-01, 0.00000000e+00, 2.05563910e-01, ...,\n",
              "        5.00000000e-01, 6.00000000e-02, 1.10000000e-04],\n",
              "       [1.06907888e-01, 1.00000000e+00, 4.09774436e-01, ...,\n",
              "        0.00000000e+00, 2.80000000e-01, 0.00000000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "El5Cdc0jMwCj",
        "colab_type": "code",
        "outputId": "17898cb8-ebe7-4dfd-b6d8-b946e1af4010",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "cell_type": "code",
      "source": [
        "X"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.5776156e+07, 1.0000000e+00, 2.2080000e+01, ..., 2.0000000e+00,\n",
              "        1.0000000e+02, 1.2130000e+03],\n",
              "       [1.5739548e+07, 0.0000000e+00, 2.2670000e+01, ..., 2.0000000e+00,\n",
              "        1.6000000e+02, 1.0000000e+00],\n",
              "       [1.5662854e+07, 0.0000000e+00, 2.9580000e+01, ..., 2.0000000e+00,\n",
              "        2.8000000e+02, 1.0000000e+00],\n",
              "       ...,\n",
              "       [1.5675450e+07, 0.0000000e+00, 1.8830000e+01, ..., 2.0000000e+00,\n",
              "        1.0000000e+02, 1.0000000e+00],\n",
              "       [1.5776494e+07, 0.0000000e+00, 2.7420000e+01, ..., 2.0000000e+00,\n",
              "        1.2000000e+02, 1.2000000e+01],\n",
              "       [1.5592412e+07, 1.0000000e+00, 4.1000000e+01, ..., 1.0000000e+00,\n",
              "        5.6000000e+02, 1.0000000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "KuOABvGnN7Bi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G0Iovd3RPmhL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*training* SOM below code will use minisom library"
      ]
    },
    {
      "metadata": {
        "id": "DolJH2bxOVIN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "\n",
        "from numpy import (array, unravel_index, nditer, linalg, random, subtract,\n",
        "                   power, exp, pi, zeros, arange, outer, meshgrid, dot)\n",
        "from collections import defaultdict\n",
        "from warnings import warn\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    Minimalistic implementation of the Self Organizing Maps (SOM).\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def fast_norm(x):\n",
        "    \"\"\"Returns norm-2 of a 1-D numpy array.\n",
        "\n",
        "    * faster than linalg.norm in case of 1-D arrays (numpy 1.9.2rc1).\n",
        "    \"\"\"\n",
        "    return sqrt(dot(x, x.T))\n",
        "\n",
        "\n",
        "class MiniSom(object):\n",
        "    def __init__(self, x, y, input_len, sigma=1.0, learning_rate=0.5, decay_function=None, random_seed=None):\n",
        "        \"\"\"\n",
        "            Initializes a Self Organizing Maps.\n",
        "            x,y - dimensions of the SOM\n",
        "            input_len - number of the elements of the vectors in input\n",
        "            sigma - spread of the neighborhood function (Gaussian), needs to be adequate to the dimensions of the map.\n",
        "            (at the iteration t we have sigma(t) = sigma / (1 + t/T) where T is #num_iteration/2)\n",
        "            learning_rate - initial learning rate\n",
        "            (at the iteration t we have learning_rate(t) = learning_rate / (1 + t/T) where T is #num_iteration/2)\n",
        "            decay_function, function that reduces learning_rate and sigma at each iteration\n",
        "                            default function: lambda x,current_iteration,max_iter: x/(1+current_iteration/max_iter)\n",
        "            random_seed, random seed to use.\n",
        "        \"\"\"\n",
        "        if sigma >= x/2.0 or sigma >= y/2.0:\n",
        "            warn('Warning: sigma is too high for the dimension of the map.')\n",
        "        if random_seed:\n",
        "            self.random_generator = random.RandomState(random_seed)\n",
        "        else:\n",
        "            self.random_generator = random.RandomState(random_seed)\n",
        "        if decay_function:\n",
        "            self._decay_function = decay_function\n",
        "        else:\n",
        "            self._decay_function = lambda x, t, max_iter: x/(1+t/max_iter)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.sigma = sigma\n",
        "        self.weights = self.random_generator.rand(x,y,input_len)*2-1 # random initialization\n",
        "        for i in range(x):\n",
        "            for j in range(y):\n",
        "                self.weights[i,j] = self.weights[i,j] / fast_norm(self.weights[i,j]) # normalization\n",
        "        self.activation_map = zeros((x,y))\n",
        "        self.neigx = arange(x)\n",
        "        self.neigy = arange(y) # used to evaluate the neighborhood function\n",
        "        self.neighborhood = self.gaussian\n",
        "\n",
        "    def _activate(self, x):\n",
        "        \"\"\" Updates matrix activation_map, in this matrix the element i,j is the response of the neuron i,j to x \"\"\"\n",
        "        s = subtract(x, self.weights) # x - w\n",
        "        it = nditer(self.activation_map, flags=['multi_index'])\n",
        "        while not it.finished:\n",
        "            self.activation_map[it.multi_index] = fast_norm(s[it.multi_index])  # || x - w ||\n",
        "            it.iternext()\n",
        "\n",
        "    def activate(self, x):\n",
        "        \"\"\" Returns the activation map to x \"\"\"\n",
        "        self._activate(x)\n",
        "        return self.activation_map\n",
        "\n",
        "    def gaussian(self, c, sigma):\n",
        "        \"\"\" Returns a Gaussian centered in c \"\"\"\n",
        "        d = 2*pi*sigma*sigma\n",
        "        ax = exp(-power(self.neigx-c[0], 2)/d)\n",
        "        ay = exp(-power(self.neigy-c[1], 2)/d)\n",
        "        return outer(ax, ay)  # the external product gives a matrix\n",
        "\n",
        "    def diff_gaussian(self, c, sigma):\n",
        "        \"\"\" Mexican hat centered in c (unused) \"\"\"\n",
        "        xx, yy = meshgrid(self.neigx, self.neigy)\n",
        "        p = power(xx-c[0], 2) + power(yy-c[1], 2)\n",
        "        d = 2*pi*sigma*sigma\n",
        "        return exp(-p/d)*(1-2/d*p)\n",
        "\n",
        "    def winner(self, x):\n",
        "        \"\"\" Computes the coordinates of the winning neuron for the sample x \"\"\"\n",
        "        self._activate(x)\n",
        "        return unravel_index(self.activation_map.argmin(), self.activation_map.shape)\n",
        "\n",
        "    def update(self, x, win, t):\n",
        "        \"\"\"\n",
        "            Updates the weights of the neurons.\n",
        "            x - current pattern to learn\n",
        "            win - position of the winning neuron for x (array or tuple).\n",
        "            t - iteration index\n",
        "        \"\"\"\n",
        "        eta = self._decay_function(self.learning_rate, t, self.T)\n",
        "        sig = self._decay_function(self.sigma, t, self.T) # sigma and learning rate decrease with the same rule\n",
        "        g = self.neighborhood(win, sig)*eta # improves the performances\n",
        "        it = nditer(g, flags=['multi_index'])\n",
        "        while not it.finished:\n",
        "            # eta * neighborhood_function * (x-w)\n",
        "            self.weights[it.multi_index] += g[it.multi_index]*(x-self.weights[it.multi_index])\n",
        "            # normalization\n",
        "            self.weights[it.multi_index] = self.weights[it.multi_index] / fast_norm(self.weights[it.multi_index])\n",
        "            it.iternext()\n",
        "\n",
        "    def quantization(self, data):\n",
        "        \"\"\" Assigns a code book (weights vector of the winning neuron) to each sample in data. \"\"\"\n",
        "        q = zeros(data.shape)\n",
        "        for i, x in enumerate(data):\n",
        "            q[i] = self.weights[self.winner(x)]\n",
        "        return q\n",
        "\n",
        "    def random_weights_init(self, data):\n",
        "        \"\"\" Initializes the weights of the SOM picking random samples from data \"\"\"\n",
        "        it = nditer(self.activation_map, flags=['multi_index'])\n",
        "        while not it.finished:\n",
        "            self.weights[it.multi_index] = data[self.random_generator.randint(len(data))]\n",
        "            self.weights[it.multi_index] = self.weights[it.multi_index]/fast_norm(self.weights[it.multi_index])\n",
        "            it.iternext()\n",
        "\n",
        "    def train_random(self, data, num_iteration):\n",
        "        \"\"\" Trains the SOM picking samples at random from data \"\"\"\n",
        "        self._init_T(num_iteration)\n",
        "        for iteration in range(num_iteration):\n",
        "            rand_i = self.random_generator.randint(len(data)) # pick a random sample\n",
        "            self.update(data[rand_i], self.winner(data[rand_i]), iteration)\n",
        "\n",
        "    def train_batch(self, data, num_iteration):\n",
        "        \"\"\" Trains using all the vectors in data sequentially \"\"\"\n",
        "        self._init_T(len(data)*num_iteration)\n",
        "        iteration = 0\n",
        "        while iteration < num_iteration:\n",
        "            idx = iteration % (len(data)-1)\n",
        "            self.update(data[idx], self.winner(data[idx]), iteration)\n",
        "            iteration += 1\n",
        "\n",
        "    def _init_T(self, num_iteration):\n",
        "        \"\"\" Initializes the parameter T needed to adjust the learning rate \"\"\"\n",
        "        self.T = num_iteration/2  # keeps the learning rate nearly constant for the last half of the iterations\n",
        "\n",
        "    def distance_map(self):\n",
        "        \"\"\" Returns the distance map of the weights.\n",
        "            Each cell is the normalised sum of the distances between a neuron and its neighbours.\n",
        "        \"\"\"\n",
        "        um = zeros((self.weights.shape[0], self.weights.shape[1]))\n",
        "        it = nditer(um, flags=['multi_index'])\n",
        "        while not it.finished:\n",
        "            for ii in range(it.multi_index[0]-1, it.multi_index[0]+2):\n",
        "                for jj in range(it.multi_index[1]-1, it.multi_index[1]+2):\n",
        "                    if ii >= 0 and ii < self.weights.shape[0] and jj >= 0 and jj < self.weights.shape[1]:\n",
        "                        um[it.multi_index] += fast_norm(self.weights[ii, jj, :]-self.weights[it.multi_index])\n",
        "            it.iternext()\n",
        "        um = um/um.max()\n",
        "        return um\n",
        "\n",
        "    def activation_response(self, data):\n",
        "        \"\"\"\n",
        "            Returns a matrix where the element i,j is the number of times\n",
        "            that the neuron i,j have been winner.\n",
        "        \"\"\"\n",
        "        a = zeros((self.weights.shape[0], self.weights.shape[1]))\n",
        "        for x in data:\n",
        "            a[self.winner(x)] += 1\n",
        "        return a\n",
        "\n",
        "    def quantization_error(self, data):\n",
        "        \"\"\"\n",
        "            Returns the quantization error computed as the average distance between\n",
        "            each input sample and its best matching unit.\n",
        "        \"\"\"\n",
        "        error = 0\n",
        "        for x in data:\n",
        "            error += fast_norm(x-self.weights[self.winner(x)])\n",
        "        return error/len(data)\n",
        "\n",
        "    def win_map(self, data):\n",
        "        \"\"\"\n",
        "            Returns a dictionary wm where wm[(i,j)] is a list with all the patterns\n",
        "            that have been mapped in the position i,j.\n",
        "        \"\"\"\n",
        "        winmap = defaultdict(list)\n",
        "        for x in data:\n",
        "            winmap[self.winner(x)].append(x)\n",
        "        return winmap\n",
        "\n",
        "### unit tests\n",
        "from numpy.testing import assert_almost_equal, assert_array_almost_equal, assert_array_equal\n",
        "\n",
        "\n",
        "class TestMinisom:\n",
        "    def setup_method(self, method):\n",
        "        self.som = MiniSom(5, 5, 1)\n",
        "        for i in range(5):\n",
        "            for j in range(5):\n",
        "                assert_almost_equal(1.0, linalg.norm(self.som.weights[i,j]))  # checking weights normalization\n",
        "        self.som.weights = zeros((5, 5))  # fake weights\n",
        "        self.som.weights[2, 3] = 5.0\n",
        "        self.som.weights[1, 1] = 2.0\n",
        "\n",
        "    def test_decay_function(self):\n",
        "        assert self.som._decay_function(1., 2., 3.) == 1./(1.+2./3.)\n",
        "\n",
        "    def test_fast_norm(self):\n",
        "        assert fast_norm(array([1, 3])) == sqrt(1+9)\n",
        "\n",
        "    def test_gaussian(self):\n",
        "        bell = self.som.gaussian((2, 2), 1)\n",
        "        assert bell.max() == 1.0\n",
        "        assert bell.argmax() == 12  # unravel(12) = (2,2)\n",
        "\n",
        "    def test_win_map(self):\n",
        "        winners = self.som.win_map([5.0, 2.0])\n",
        "        assert winners[(2, 3)][0] == 5.0\n",
        "        assert winners[(1, 1)][0] == 2.0\n",
        "\n",
        "    def test_activation_reponse(self):\n",
        "        response = self.som.activation_response([5.0, 2.0])\n",
        "        assert response[2, 3] == 1\n",
        "        assert response[1, 1] == 1\n",
        "\n",
        "    def test_activate(self):\n",
        "        assert self.som.activate(5.0).argmin() == 13.0  # unravel(13) = (2,3)\n",
        "\n",
        "    def test_quantization_error(self):\n",
        "        self.som.quantization_error([5, 2]) == 0.0\n",
        "        self.som.quantization_error([4, 1]) == 0.5\n",
        "\n",
        "    def test_quantization(self):\n",
        "        q = self.som.quantization(array([4, 2]))\n",
        "        assert q[0] == 5.0\n",
        "        assert q[1] == 2.0\n",
        "\n",
        "    def test_random_seed(self):\n",
        "        som1 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
        "        som2 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
        "        assert_array_almost_equal(som1.weights, som2.weights)  # same initialization\n",
        "        data = random.rand(100,2)\n",
        "        som1 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
        "        som1.train_random(data,10)\n",
        "        som2 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
        "        som2.train_random(data,10)\n",
        "        assert_array_almost_equal(som1.weights,som2.weights)  # same state after training\n",
        "\n",
        "    def test_train_batch(self):\n",
        "        som = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
        "        data = array([[4, 2], [3, 1]])\n",
        "        q1 = som.quantization_error(data)\n",
        "        som.train_batch(data, 10)\n",
        "        assert q1 > som.quantization_error(data)\n",
        "\n",
        "    def test_train_random(self):\n",
        "        som = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
        "        data = array([[4, 2], [3, 1]])\n",
        "        q1 = som.quantization_error(data)\n",
        "        som.train_random(data, 10)\n",
        "        assert q1 > som.quantization_error(data)\n",
        "\n",
        "    def test_random_weights_init(self):\n",
        "        som = MiniSom(2, 2, 2, random_seed=1)\n",
        "        som.random_weights_init(array([[1.0, .0]]))\n",
        "        for w in som.weights:\n",
        "            assert_array_equal(w[0], array([1.0, .0]))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bSUXPCtmPem1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "som=MiniSom(x=10,y=10,input_len=15,sigma=1.0,learning_rate=0.5) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u1cyANwiQJ7i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "som.random_weights_init(X)\n",
        "som.train_random(X,100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RgFvhtxZSjlL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pylab import bone, pcolor, colorbar, plot,show"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xST4HNz9UOns",
        "colab_type": "code",
        "outputId": "95fa151b-0e39-42f6-e545-7a59029fd995",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "cell_type": "code",
      "source": [
        "bone()\n",
        "pcolor(som.distance_map().T)\n",
        "colorbar()\n",
        "markers=['o','s']\n",
        "colors=['r','g'] \n",
        "for i,x in enumerate(X):\n",
        "  w=som.winner(x)\n",
        "  plot(w[0]+0.5,\n",
        "       w[1]+0.5,\n",
        "       markers[y[i]],\n",
        "       markeredgecolor=colors[y[i]],\n",
        "       markerfacecolor=None,\n",
        "       markersize=10,\n",
        "       markeredgewidth=2)\n",
        "show()#to put the marker at the center of the square"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcAAAAFOCAYAAAARn83bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "wdJjJBoeUQZg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M59ZjWxLaS11",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "finding the frauds"
      ]
    },
    {
      "metadata": {
        "id": "KK54upREaVZU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mappings=som.win_map(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dwhmDEt9aZb3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "frauds= mappings[(5,4)]#because of relatively white color and having both square and red."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SZh6ccjoajJX",
        "colab_type": "code",
        "outputId": "03410a0b-a496-4561-96b4-20b9e639730a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        }
      },
      "cell_type": "code",
      "source": [
        "print(sc.inverse_transform(frauds))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3.93414409e+12 1.00000000e+00 1.79262500e+03 3.15000000e+01\n",
            "  5.00000000e+00 1.83000000e+02 6.50000000e+01 3.56250000e+01\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  5.00000000e+00 0.00000000e+00 5.29900001e+08]\n",
            " [3.89339906e+12 0.00000000e+00 1.67625000e+03 2.45000000e+01\n",
            "  5.00000000e+00 1.83000000e+02 6.50000000e+01 2.96400000e+01\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  5.00000000e+00 3.20000000e+05 5.86100001e+08]\n",
            " [3.91838644e+12 1.00000000e+00 1.69819500e+03 1.62400000e+01\n",
            "  5.00000000e+00 1.05000000e+02 3.30000000e+01 8.26500000e+00\n",
            "  1.00000000e+00 1.00000000e+00 4.69000000e+02 1.00000000e+00\n",
            "  5.00000000e+00 1.92000000e+05 5.12500001e+08]\n",
            " [3.93347732e+12 1.00000000e+00 3.30550000e+03 2.12380000e+02\n",
            "  5.00000000e+00 4.00000000e+01 4.10000000e+01 2.16172500e+02\n",
            "  1.00000000e+00 1.00000000e+00 1.00500000e+03 1.00000000e+00\n",
            "  5.00000000e+00 0.00000000e+00 5.00100001e+08]\n",
            " [3.93175993e+12 1.00000000e+00 1.55987500e+03 3.53500000e+02\n",
            "  5.00000000e+00 1.05000000e+02 3.30000000e+01 3.56250000e+00\n",
            "  0.00000000e+00 1.00000000e+00 1.34000000e+02 0.00000000e+00\n",
            "  5.00000000e+00 0.00000000e+00 5.55300001e+08]\n",
            " [3.92988422e+12 0.00000000e+00 2.11315500e+03 3.15000000e+02\n",
            "  5.00000000e+00 1.40000000e+01 9.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  5.00000000e+00 3.68000000e+05 5.20100001e+08]\n",
            " [3.90741610e+12 1.00000000e+00 1.78730500e+03 4.90000000e+01\n",
            "  3.00000000e+00 1.05000000e+02 3.30000000e+01 2.85000000e+01\n",
            "  1.00000000e+00 1.00000000e+00 3.35000000e+02 1.00000000e+00\n",
            "  5.00000000e+00 3.20000000e+05 5.77800001e+08]\n",
            " [3.93918812e+12 1.00000000e+00 2.50218000e+03 5.71200000e+01\n",
            "  5.00000000e+00 1.18000000e+02 3.30000000e+01 1.14000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  5.00000000e+00 8.00000000e+05 5.80100001e+08]\n",
            " [3.88800117e+12 1.00000000e+00 2.09719500e+03 5.46000000e+02\n",
            "  5.00000000e+00 1.05000000e+02 3.30000000e+01 1.99500000e+02\n",
            "  1.00000000e+00 1.00000000e+00 1.07200000e+03 0.00000000e+00\n",
            "  5.00000000e+00 0.00000000e+00 5.00100001e+08]\n",
            " [3.94367825e+12 0.00000000e+00 1.80393000e+03 3.78000000e+02\n",
            "  5.00000000e+00 1.44000000e+02 6.50000000e+01 1.42500000e+02\n",
            "  1.00000000e+00 1.00000000e+00 1.34000000e+02 0.00000000e+00\n",
            "  5.00000000e+00 0.00000000e+00 5.00100001e+08]\n",
            " [3.88868842e+12 1.00000000e+00 2.61855500e+03 4.55000000e+01\n",
            "  5.00000000e+00 1.05000000e+02 3.30000000e+01 4.27500000e+01\n",
            "  1.00000000e+00 1.00000000e+00 6.70000000e+02 0.00000000e+00\n",
            "  5.00000000e+00 3.72000000e+05 4.70100001e+08]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y0vlX3pxb6Ym",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}